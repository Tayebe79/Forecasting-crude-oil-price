{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fc6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#welcome! you can just click on run.\n",
    "#code is runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc161951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import  train_test_split\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70dd04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('crude oil dataset/MAIN - Copy (2).xlsx')\n",
    "train = data\n",
    "#test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa61c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>WTI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>48.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>45.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>46.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>48.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>49.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date    WTI\n",
       "0 2017-05-01  48.48\n",
       "1 2017-06-01  45.18\n",
       "2 2017-07-01  46.63\n",
       "3 2017-08-01  48.04\n",
       "4 2017-09-01  49.82"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gp = train.sort_values('Date').groupby(['DJIA','GDP', 'Date'], as_index=False)\n",
    "train_gp = train_gp.agg({'WTI':['mean']})\n",
    "train_gp.columns = ['DJIA','GDP', 'Date', 'WTI']\n",
    "train_gp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5b4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
    "    cols, names = list(), list()\n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(window, 0, -1):\n",
    "        cols.append(data.shift(i))\n",
    "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    # Current timestep (t=0)\n",
    "    cols.append(data)\n",
    "    names += [('%s(t)' % (col)) for col in data.columns]\n",
    "    # Target timestep (t=lag)\n",
    "    cols.append(data.shift(-lag))\n",
    "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    # Put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7eeed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WTI(t-2)</th>\n",
       "      <th>WTI(t-1)</th>\n",
       "      <th>WTI(t)</th>\n",
       "      <th>WTI(t+8)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>61.72</td>\n",
       "      <td>65.17</td>\n",
       "      <td>71.38</td>\n",
       "      <td>91.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>65.17</td>\n",
       "      <td>71.38</td>\n",
       "      <td>72.49</td>\n",
       "      <td>108.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>71.38</td>\n",
       "      <td>72.49</td>\n",
       "      <td>67.73</td>\n",
       "      <td>101.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>72.49</td>\n",
       "      <td>67.73</td>\n",
       "      <td>71.65</td>\n",
       "      <td>109.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>67.73</td>\n",
       "      <td>71.65</td>\n",
       "      <td>81.48</td>\n",
       "      <td>114.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    WTI(t-2)  WTI(t-1)  WTI(t)  WTI(t+8)\n",
       "49     61.72     65.17   71.38     91.64\n",
       "50     65.17     71.38   72.49    108.50\n",
       "51     71.38     72.49   67.73    101.78\n",
       "52     72.49     67.73   71.65    109.55\n",
       "53     67.73     71.65   81.48    114.84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 2\n",
    "lag = 8\n",
    "lag_size = 8\n",
    "series = series_to_supervised(train_gp.drop('Date', axis=1), window=window, lag=lag)\n",
    "series.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c8cd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.dropna of     WTI(t-2)  WTI(t-1)  WTI(t)  WTI(t+8)\n",
       "2      48.48     45.18   46.63     62.73\n",
       "3      45.18     46.63   48.04     66.25\n",
       "4      46.63     48.04   49.82     69.98\n",
       "5      48.04     49.82   51.58     67.87\n",
       "6      49.82     51.58   56.64     70.98\n",
       "7      51.58     56.64   57.88     68.06\n",
       "8      56.64     57.88   63.70     70.23\n",
       "9      57.88     63.70   62.23     70.75\n",
       "10     63.70     62.23   62.73     56.96\n",
       "11     62.23     62.73   66.25     49.52\n",
       "12     62.73     66.25   69.98     51.38\n",
       "13     66.25     69.98   67.87     54.95\n",
       "14     69.98     67.87   70.98     58.15\n",
       "15     67.87     70.98   68.06     63.86\n",
       "16     70.98     68.06   70.23     60.83\n",
       "17     68.06     70.23   70.75     54.66\n",
       "18     70.23     70.75   56.96     57.35\n",
       "19     70.75     56.96   49.52     54.81\n",
       "20     56.96     49.52   51.38     56.95\n",
       "21     49.52     51.38   54.95     53.96\n",
       "22     51.38     54.95   58.15     57.03\n",
       "23     54.95     58.15   63.86     59.88\n",
       "24     58.15     63.86   60.83     57.52\n",
       "25     63.86     60.83   54.66     50.54\n",
       "26     60.83     54.66   57.35     29.21\n",
       "27     54.66     57.35   54.81     16.55\n",
       "28     57.35     54.81   56.95     28.56\n",
       "29     54.81     56.95   53.96     38.31\n",
       "30     56.95     53.96   57.03     40.71\n",
       "31     53.96     57.03   59.88     42.34\n",
       "32     57.03     59.88   57.52     39.63\n",
       "33     59.88     57.52   50.54     39.40\n",
       "34     57.52     50.54   29.21     40.94\n",
       "35     50.54     29.21   16.55     47.02\n",
       "36     29.21     16.55   28.56     52.00\n",
       "37     16.55     28.56   38.31     59.04\n",
       "38     28.56     38.31   40.71     62.33\n",
       "39     38.31     40.71   42.34     61.72\n",
       "40     40.71     42.34   39.63     65.17\n",
       "41     42.34     39.63   39.40     71.38\n",
       "42     39.63     39.40   40.94     72.49\n",
       "43     39.40     40.94   47.02     67.73\n",
       "44     40.94     47.02   52.00     71.65\n",
       "45     47.02     52.00   59.04     81.48\n",
       "46     52.00     59.04   62.33     79.15\n",
       "47     59.04     62.33   61.72     71.71\n",
       "48     62.33     61.72   65.17     83.22\n",
       "49     61.72     65.17   71.38     91.64\n",
       "50     65.17     71.38   72.49    108.50\n",
       "51     71.38     72.49   67.73    101.78\n",
       "52     72.49     67.73   71.65    109.55\n",
       "53     67.73     71.65   81.48    114.84>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f9e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1607457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    WTI(t-2)  WTI(t-1)  WTI(t)  WTI(t+8)\n",
      "2      48.48     45.18   46.63     62.73\n",
      "3      45.18     46.63   48.04     66.25\n",
      "4      46.63     48.04   49.82     69.98\n",
      "5      48.04     49.82   51.58     67.87\n",
      "6      49.82     51.58   56.64     70.98\n",
      "7      51.58     56.64   57.88     68.06\n",
      "8      56.64     57.88   63.70     70.23\n",
      "9      57.88     63.70   62.23     70.75\n",
      "10     63.70     62.23   62.73     56.96\n",
      "11     62.23     62.73   66.25     49.52\n",
      "12     62.73     66.25   69.98     51.38\n",
      "13     66.25     69.98   67.87     54.95\n",
      "14     69.98     67.87   70.98     58.15\n",
      "15     67.87     70.98   68.06     63.86\n",
      "16     70.98     68.06   70.23     60.83\n",
      "17     68.06     70.23   70.75     54.66\n",
      "18     70.23     70.75   56.96     57.35\n",
      "19     70.75     56.96   49.52     54.81\n",
      "20     56.96     49.52   51.38     56.95\n",
      "21     49.52     51.38   54.95     53.96\n",
      "22     51.38     54.95   58.15     57.03\n",
      "23     54.95     58.15   63.86     59.88\n",
      "24     58.15     63.86   60.83     57.52\n",
      "25     63.86     60.83   54.66     50.54\n",
      "26     60.83     54.66   57.35     29.21\n",
      "27     54.66     57.35   54.81     16.55\n",
      "28     57.35     54.81   56.95     28.56\n",
      "29     54.81     56.95   53.96     38.31\n",
      "30     56.95     53.96   57.03     40.71\n",
      "31     53.96     57.03   59.88     42.34\n",
      "32     57.03     59.88   57.52     39.63\n",
      "33     59.88     57.52   50.54     39.40\n",
      "34     57.52     50.54   29.21     40.94\n",
      "35     50.54     29.21   16.55     47.02\n",
      "36     29.21     16.55   28.56     52.00\n",
      "37     16.55     28.56   38.31     59.04\n",
      "38     28.56     38.31   40.71     62.33\n",
      "39     38.31     40.71   42.34     61.72\n",
      "40     40.71     42.34   39.63     65.17\n",
      "41     42.34     39.63   39.40     71.38\n",
      "42     39.63     39.40   40.94     72.49\n",
      "43     39.40     40.94   47.02     67.73\n",
      "44     40.94     47.02   52.00     71.65\n",
      "45     47.02     52.00   59.04     81.48\n",
      "46     52.00     59.04   62.33     79.15\n",
      "47     59.04     62.33   61.72     71.71\n",
      "48     62.33     61.72   65.17     83.22\n",
      "49     61.72     65.17   71.38     91.64\n",
      "50     65.17     71.38   72.49    108.50\n",
      "51     71.38     72.49   67.73    101.78\n",
      "52     72.49     67.73   71.65    109.55\n",
      "53     67.73     71.65   81.48    114.84\n"
     ]
    }
   ],
   "source": [
    "#last_item = 'DJIA(t-%d)' % window\n",
    "#last_store = 'GDP(t-%d)' % window\n",
    "#series = series[(series['GDP(t+%d)' % lag] == series[last_store])]\n",
    "#series = series[(series['DJIA(t+%d)'  % lag] == series[last_item ])]\n",
    "#series = series[(series['GDP(t)'] == series[last_store])]\n",
    "#series = series[(series['DJIA(t)'] == series[last_item])]\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce09b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['DJIA','GDP']]\n",
    "#for i in range(window, 0, -1):\n",
    "#    columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['DJIA','GDP']]\n",
    "#series.drop(columns_to_drop, axis=1, inplace=True)\n",
    "#series.drop(['DJIA(t)','GDP(t)'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a2682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    WTI(t-2)  WTI(t-1)  WTI(t)  WTI(t+8)\n",
      "2      48.48     45.18   46.63     62.73\n",
      "3      45.18     46.63   48.04     66.25\n",
      "4      46.63     48.04   49.82     69.98\n",
      "5      48.04     49.82   51.58     67.87\n",
      "6      49.82     51.58   56.64     70.98\n",
      "7      51.58     56.64   57.88     68.06\n",
      "8      56.64     57.88   63.70     70.23\n",
      "9      57.88     63.70   62.23     70.75\n",
      "10     63.70     62.23   62.73     56.96\n",
      "11     62.23     62.73   66.25     49.52\n",
      "12     62.73     66.25   69.98     51.38\n",
      "13     66.25     69.98   67.87     54.95\n",
      "14     69.98     67.87   70.98     58.15\n",
      "15     67.87     70.98   68.06     63.86\n",
      "16     70.98     68.06   70.23     60.83\n",
      "17     68.06     70.23   70.75     54.66\n",
      "18     70.23     70.75   56.96     57.35\n",
      "19     70.75     56.96   49.52     54.81\n",
      "20     56.96     49.52   51.38     56.95\n",
      "21     49.52     51.38   54.95     53.96\n",
      "22     51.38     54.95   58.15     57.03\n",
      "23     54.95     58.15   63.86     59.88\n",
      "24     58.15     63.86   60.83     57.52\n",
      "25     63.86     60.83   54.66     50.54\n",
      "26     60.83     54.66   57.35     29.21\n",
      "27     54.66     57.35   54.81     16.55\n",
      "28     57.35     54.81   56.95     28.56\n",
      "29     54.81     56.95   53.96     38.31\n",
      "30     56.95     53.96   57.03     40.71\n",
      "31     53.96     57.03   59.88     42.34\n",
      "32     57.03     59.88   57.52     39.63\n",
      "33     59.88     57.52   50.54     39.40\n",
      "34     57.52     50.54   29.21     40.94\n",
      "35     50.54     29.21   16.55     47.02\n",
      "36     29.21     16.55   28.56     52.00\n",
      "37     16.55     28.56   38.31     59.04\n",
      "38     28.56     38.31   40.71     62.33\n",
      "39     38.31     40.71   42.34     61.72\n",
      "40     40.71     42.34   39.63     65.17\n",
      "41     42.34     39.63   39.40     71.38\n",
      "42     39.63     39.40   40.94     72.49\n",
      "43     39.40     40.94   47.02     67.73\n",
      "44     40.94     47.02   52.00     71.65\n",
      "45     47.02     52.00   59.04     81.48\n",
      "46     52.00     59.04   62.33     79.15\n",
      "47     59.04     62.33   61.72     71.71\n",
      "48     62.33     61.72   65.17     83.22\n",
      "49     61.72     65.17   71.38     91.64\n",
      "50     65.17     71.38   72.49    108.50\n",
      "51     71.38     72.49   67.73    101.78\n",
      "52     72.49     67.73   71.65    109.55\n",
      "53     67.73     71.65   81.48    114.84\n"
     ]
    }
   ],
   "source": [
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ce106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape (39, 3)\n",
      "Validation set shape (13, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WTI(t-2)</th>\n",
       "      <th>WTI(t-1)</th>\n",
       "      <th>WTI(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>53.96</td>\n",
       "      <td>57.03</td>\n",
       "      <td>59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>40.94</td>\n",
       "      <td>47.02</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>47.02</td>\n",
       "      <td>52.00</td>\n",
       "      <td>59.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57.88</td>\n",
       "      <td>63.70</td>\n",
       "      <td>62.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>70.98</td>\n",
       "      <td>68.06</td>\n",
       "      <td>70.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    WTI(t-2)  WTI(t-1)  WTI(t)\n",
       "31     53.96     57.03   59.88\n",
       "44     40.94     47.02   52.00\n",
       "45     47.02     52.00   59.04\n",
       "9      57.88     63.70   62.23\n",
       "16     70.98     68.06   70.23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_col = 'WTI(t+%d)' % lag_size\n",
    "labels = series[labels_col]\n",
    "series = series.drop(labels_col, axis=1)\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.25, random_state=0)\n",
    "#X_train = series[:24]\n",
    "#Y_train = labels.values[:24]\n",
    "#X_valid = series[24:]\n",
    "#Y_valid = labels.values[24:]\n",
    "print('Train set shape', X_train.shape)\n",
    "print('Validation set shape', X_valid.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd55dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75943426",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "#batch = 256\n",
    "lr = 0.005\n",
    "adam = tf.keras.optimizers.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79cfee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#model_mlp = tf.keras.Sequential()\n",
    "#model_mlp.add(Dense(3, activation='relu', input_dim=X_train.shape[1]))\n",
    "#tf.keras.layers.Dense(3, activation='relu',input_dim=2),\n",
    "#model_mlp.add(Dense(1))\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Dense(3, activation='relu',input_dim=X_train.shape[1]),\n",
    "tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56d246b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 - 0s - loss: 28.9657 - val_loss: 16.9964 - 486ms/epoch - 243ms/step\n",
      "Epoch 2/200\n",
      "2/2 - 0s - loss: 25.4052 - val_loss: 16.4052 - 19ms/epoch - 10ms/step\n",
      "Epoch 3/200\n",
      "2/2 - 0s - loss: 22.2657 - val_loss: 17.1785 - 21ms/epoch - 10ms/step\n",
      "Epoch 4/200\n",
      "2/2 - 0s - loss: 20.2197 - val_loss: 18.4472 - 23ms/epoch - 12ms/step\n",
      "Epoch 5/200\n",
      "2/2 - 0s - loss: 18.8504 - val_loss: 19.7028 - 22ms/epoch - 11ms/step\n",
      "Epoch 6/200\n",
      "2/2 - 0s - loss: 17.9601 - val_loss: 20.8932 - 22ms/epoch - 11ms/step\n",
      "Epoch 7/200\n",
      "2/2 - 0s - loss: 17.4731 - val_loss: 21.9915 - 21ms/epoch - 11ms/step\n",
      "Epoch 8/200\n",
      "2/2 - 0s - loss: 17.3729 - val_loss: 22.9378 - 21ms/epoch - 10ms/step\n",
      "Epoch 9/200\n",
      "2/2 - 0s - loss: 17.3879 - val_loss: 23.6720 - 22ms/epoch - 11ms/step\n",
      "Epoch 10/200\n",
      "2/2 - 0s - loss: 17.6949 - val_loss: 24.1231 - 22ms/epoch - 11ms/step\n",
      "Epoch 11/200\n",
      "2/2 - 0s - loss: 17.7821 - val_loss: 24.2556 - 20ms/epoch - 10ms/step\n",
      "Epoch 12/200\n",
      "2/2 - 0s - loss: 17.8255 - val_loss: 24.2271 - 22ms/epoch - 11ms/step\n",
      "Epoch 13/200\n",
      "2/2 - 0s - loss: 17.8087 - val_loss: 24.0870 - 20ms/epoch - 10ms/step\n",
      "Epoch 14/200\n",
      "2/2 - 0s - loss: 17.7422 - val_loss: 23.8574 - 20ms/epoch - 10ms/step\n",
      "Epoch 15/200\n",
      "2/2 - 0s - loss: 17.6406 - val_loss: 23.5454 - 21ms/epoch - 11ms/step\n",
      "Epoch 16/200\n",
      "2/2 - 0s - loss: 17.4959 - val_loss: 23.1599 - 21ms/epoch - 11ms/step\n",
      "Epoch 17/200\n",
      "2/2 - 0s - loss: 17.3948 - val_loss: 22.7469 - 21ms/epoch - 11ms/step\n",
      "Epoch 18/200\n",
      "2/2 - 0s - loss: 17.3308 - val_loss: 22.3132 - 20ms/epoch - 10ms/step\n",
      "Epoch 19/200\n",
      "2/2 - 0s - loss: 17.2835 - val_loss: 21.8919 - 21ms/epoch - 10ms/step\n",
      "Epoch 20/200\n",
      "2/2 - 0s - loss: 17.3313 - val_loss: 21.5462 - 21ms/epoch - 10ms/step\n",
      "Epoch 21/200\n",
      "2/2 - 0s - loss: 17.3364 - val_loss: 21.3341 - 22ms/epoch - 11ms/step\n",
      "Epoch 22/200\n",
      "2/2 - 0s - loss: 17.4097 - val_loss: 21.1676 - 21ms/epoch - 11ms/step\n",
      "Epoch 23/200\n",
      "2/2 - 0s - loss: 17.4561 - val_loss: 21.1408 - 22ms/epoch - 11ms/step\n",
      "Epoch 24/200\n",
      "2/2 - 0s - loss: 17.4496 - val_loss: 21.2451 - 21ms/epoch - 11ms/step\n",
      "Epoch 25/200\n",
      "2/2 - 0s - loss: 17.3980 - val_loss: 21.4847 - 21ms/epoch - 10ms/step\n",
      "Epoch 26/200\n",
      "2/2 - 0s - loss: 17.3076 - val_loss: 21.9069 - 22ms/epoch - 11ms/step\n",
      "Epoch 27/200\n",
      "2/2 - 0s - loss: 17.2620 - val_loss: 22.3373 - 21ms/epoch - 10ms/step\n",
      "Epoch 28/200\n",
      "2/2 - 0s - loss: 17.3191 - val_loss: 22.6445 - 21ms/epoch - 10ms/step\n",
      "Epoch 29/200\n",
      "2/2 - 0s - loss: 17.3126 - val_loss: 22.7321 - 20ms/epoch - 10ms/step\n",
      "Epoch 30/200\n",
      "2/2 - 0s - loss: 17.3178 - val_loss: 22.7871 - 21ms/epoch - 10ms/step\n",
      "Epoch 31/200\n",
      "2/2 - 0s - loss: 17.3085 - val_loss: 22.9400 - 22ms/epoch - 11ms/step\n",
      "Epoch 32/200\n",
      "2/2 - 0s - loss: 17.3220 - val_loss: 23.1466 - 22ms/epoch - 11ms/step\n",
      "Epoch 33/200\n",
      "2/2 - 0s - loss: 17.3799 - val_loss: 23.2853 - 21ms/epoch - 10ms/step\n",
      "Epoch 34/200\n",
      "2/2 - 0s - loss: 17.3980 - val_loss: 23.2662 - 21ms/epoch - 11ms/step\n",
      "Epoch 35/200\n",
      "2/2 - 0s - loss: 17.3868 - val_loss: 23.1269 - 21ms/epoch - 10ms/step\n",
      "Epoch 36/200\n",
      "2/2 - 0s - loss: 17.3706 - val_loss: 22.9015 - 21ms/epoch - 10ms/step\n",
      "Epoch 37/200\n",
      "2/2 - 0s - loss: 17.3012 - val_loss: 22.7125 - 21ms/epoch - 11ms/step\n",
      "Epoch 38/200\n",
      "2/2 - 0s - loss: 17.2497 - val_loss: 22.3795 - 22ms/epoch - 11ms/step\n",
      "Epoch 39/200\n",
      "2/2 - 0s - loss: 17.2302 - val_loss: 21.9445 - 21ms/epoch - 10ms/step\n",
      "Epoch 40/200\n",
      "2/2 - 0s - loss: 17.2493 - val_loss: 21.6484 - 21ms/epoch - 11ms/step\n",
      "Epoch 41/200\n",
      "2/2 - 0s - loss: 17.2278 - val_loss: 21.5705 - 22ms/epoch - 11ms/step\n",
      "Epoch 42/200\n",
      "2/2 - 0s - loss: 17.2201 - val_loss: 21.6175 - 21ms/epoch - 11ms/step\n",
      "Epoch 43/200\n",
      "2/2 - 0s - loss: 17.2228 - val_loss: 21.6285 - 24ms/epoch - 12ms/step\n",
      "Epoch 44/200\n",
      "2/2 - 0s - loss: 17.2126 - val_loss: 21.6263 - 20ms/epoch - 10ms/step\n",
      "Epoch 45/200\n",
      "2/2 - 0s - loss: 17.2096 - val_loss: 21.7019 - 21ms/epoch - 11ms/step\n",
      "Epoch 46/200\n",
      "2/2 - 0s - loss: 17.1884 - val_loss: 21.6708 - 19ms/epoch - 10ms/step\n",
      "Epoch 47/200\n",
      "2/2 - 0s - loss: 17.1841 - val_loss: 21.6487 - 22ms/epoch - 11ms/step\n",
      "Epoch 48/200\n",
      "2/2 - 0s - loss: 17.1945 - val_loss: 21.6128 - 20ms/epoch - 10ms/step\n",
      "Epoch 49/200\n",
      "2/2 - 0s - loss: 17.1939 - val_loss: 21.5436 - 23ms/epoch - 12ms/step\n",
      "Epoch 50/200\n",
      "2/2 - 0s - loss: 17.1934 - val_loss: 21.6872 - 21ms/epoch - 10ms/step\n",
      "Epoch 51/200\n",
      "2/2 - 0s - loss: 17.2054 - val_loss: 21.8969 - 24ms/epoch - 12ms/step\n",
      "Epoch 52/200\n",
      "2/2 - 0s - loss: 17.1661 - val_loss: 21.9351 - 23ms/epoch - 11ms/step\n",
      "Epoch 53/200\n",
      "2/2 - 0s - loss: 17.1624 - val_loss: 21.8432 - 21ms/epoch - 10ms/step\n",
      "Epoch 54/200\n",
      "2/2 - 0s - loss: 17.1342 - val_loss: 21.5696 - 21ms/epoch - 11ms/step\n",
      "Epoch 55/200\n",
      "2/2 - 0s - loss: 17.1704 - val_loss: 21.2578 - 20ms/epoch - 10ms/step\n",
      "Epoch 56/200\n",
      "2/2 - 0s - loss: 17.2553 - val_loss: 21.1467 - 20ms/epoch - 10ms/step\n",
      "Epoch 57/200\n",
      "2/2 - 0s - loss: 17.2589 - val_loss: 21.2267 - 141ms/epoch - 71ms/step\n",
      "Epoch 58/200\n",
      "2/2 - 0s - loss: 17.2308 - val_loss: 21.3773 - 28ms/epoch - 14ms/step\n",
      "Epoch 59/200\n",
      "2/2 - 0s - loss: 17.2239 - val_loss: 21.4733 - 21ms/epoch - 11ms/step\n",
      "Epoch 60/200\n",
      "2/2 - 0s - loss: 17.1569 - val_loss: 21.3842 - 21ms/epoch - 11ms/step\n",
      "Epoch 61/200\n",
      "2/2 - 0s - loss: 17.1636 - val_loss: 21.2571 - 21ms/epoch - 11ms/step\n",
      "Epoch 62/200\n",
      "2/2 - 0s - loss: 17.2025 - val_loss: 21.1688 - 21ms/epoch - 11ms/step\n",
      "Epoch 63/200\n",
      "2/2 - 0s - loss: 17.2195 - val_loss: 21.1884 - 20ms/epoch - 10ms/step\n",
      "Epoch 64/200\n",
      "2/2 - 0s - loss: 17.2043 - val_loss: 21.3053 - 21ms/epoch - 11ms/step\n",
      "Epoch 65/200\n",
      "2/2 - 0s - loss: 17.1531 - val_loss: 21.5415 - 20ms/epoch - 10ms/step\n",
      "Epoch 66/200\n",
      "2/2 - 0s - loss: 17.1066 - val_loss: 21.8478 - 20ms/epoch - 10ms/step\n",
      "Epoch 67/200\n",
      "2/2 - 0s - loss: 17.1018 - val_loss: 22.1102 - 20ms/epoch - 10ms/step\n",
      "Epoch 68/200\n",
      "2/2 - 0s - loss: 17.0896 - val_loss: 22.2645 - 20ms/epoch - 10ms/step\n",
      "Epoch 69/200\n",
      "2/2 - 0s - loss: 17.1086 - val_loss: 22.3484 - 21ms/epoch - 10ms/step\n",
      "Epoch 70/200\n",
      "2/2 - 0s - loss: 17.1185 - val_loss: 22.3302 - 20ms/epoch - 10ms/step\n",
      "Epoch 71/200\n",
      "2/2 - 0s - loss: 17.1013 - val_loss: 22.1230 - 21ms/epoch - 11ms/step\n",
      "Epoch 72/200\n",
      "2/2 - 0s - loss: 17.0700 - val_loss: 21.7738 - 20ms/epoch - 10ms/step\n",
      "Epoch 73/200\n",
      "2/2 - 0s - loss: 17.0633 - val_loss: 21.4906 - 20ms/epoch - 10ms/step\n",
      "Epoch 74/200\n",
      "2/2 - 0s - loss: 17.0616 - val_loss: 21.2458 - 21ms/epoch - 11ms/step\n",
      "Epoch 75/200\n",
      "2/2 - 0s - loss: 17.1597 - val_loss: 21.1309 - 26ms/epoch - 13ms/step\n",
      "Epoch 76/200\n",
      "2/2 - 0s - loss: 17.1448 - val_loss: 21.2372 - 21ms/epoch - 11ms/step\n",
      "Epoch 77/200\n",
      "2/2 - 0s - loss: 17.0973 - val_loss: 21.4855 - 21ms/epoch - 11ms/step\n",
      "Epoch 78/200\n",
      "2/2 - 0s - loss: 17.0482 - val_loss: 21.9024 - 22ms/epoch - 11ms/step\n",
      "Epoch 79/200\n",
      "2/2 - 0s - loss: 17.0481 - val_loss: 22.1715 - 21ms/epoch - 11ms/step\n",
      "Epoch 80/200\n",
      "2/2 - 0s - loss: 17.0404 - val_loss: 22.3115 - 22ms/epoch - 11ms/step\n",
      "Epoch 81/200\n",
      "2/2 - 0s - loss: 17.0665 - val_loss: 22.3895 - 21ms/epoch - 10ms/step\n",
      "Epoch 82/200\n",
      "2/2 - 0s - loss: 17.0671 - val_loss: 22.3435 - 21ms/epoch - 10ms/step\n",
      "Epoch 83/200\n",
      "2/2 - 0s - loss: 17.0518 - val_loss: 22.1916 - 22ms/epoch - 11ms/step\n",
      "Epoch 84/200\n",
      "2/2 - 0s - loss: 17.0281 - val_loss: 21.9731 - 20ms/epoch - 10ms/step\n",
      "Epoch 85/200\n",
      "2/2 - 0s - loss: 17.0095 - val_loss: 21.8502 - 20ms/epoch - 10ms/step\n",
      "Epoch 86/200\n",
      "2/2 - 0s - loss: 16.9872 - val_loss: 21.8022 - 21ms/epoch - 11ms/step\n",
      "Epoch 87/200\n",
      "2/2 - 0s - loss: 16.9769 - val_loss: 21.6671 - 20ms/epoch - 10ms/step\n",
      "Epoch 88/200\n",
      "2/2 - 0s - loss: 16.9707 - val_loss: 21.4678 - 20ms/epoch - 10ms/step\n",
      "Epoch 89/200\n",
      "2/2 - 0s - loss: 16.9934 - val_loss: 21.3004 - 20ms/epoch - 10ms/step\n",
      "Epoch 90/200\n",
      "2/2 - 0s - loss: 17.0078 - val_loss: 21.1828 - 20ms/epoch - 10ms/step\n",
      "Epoch 91/200\n",
      "2/2 - 0s - loss: 17.0494 - val_loss: 21.1807 - 21ms/epoch - 11ms/step\n",
      "Epoch 92/200\n",
      "2/2 - 0s - loss: 17.0326 - val_loss: 21.3280 - 21ms/epoch - 11ms/step\n",
      "Epoch 93/200\n",
      "2/2 - 0s - loss: 17.0203 - val_loss: 21.3123 - 20ms/epoch - 10ms/step\n",
      "Epoch 94/200\n",
      "2/2 - 0s - loss: 16.9876 - val_loss: 21.1608 - 20ms/epoch - 10ms/step\n",
      "Epoch 95/200\n",
      "2/2 - 0s - loss: 17.0126 - val_loss: 21.1766 - 20ms/epoch - 10ms/step\n",
      "Epoch 96/200\n",
      "2/2 - 0s - loss: 16.9815 - val_loss: 21.4553 - 21ms/epoch - 11ms/step\n",
      "Epoch 97/200\n",
      "2/2 - 0s - loss: 17.0194 - val_loss: 21.7993 - 21ms/epoch - 10ms/step\n",
      "Epoch 98/200\n",
      "2/2 - 0s - loss: 16.9445 - val_loss: 21.8240 - 20ms/epoch - 10ms/step\n",
      "Epoch 99/200\n",
      "2/2 - 0s - loss: 16.9182 - val_loss: 21.6323 - 20ms/epoch - 10ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "2/2 - 0s - loss: 16.9455 - val_loss: 21.4100 - 20ms/epoch - 10ms/step\n",
      "Epoch 101/200\n",
      "2/2 - 0s - loss: 16.9409 - val_loss: 21.3510 - 20ms/epoch - 10ms/step\n",
      "Epoch 102/200\n",
      "2/2 - 0s - loss: 16.9457 - val_loss: 21.3142 - 19ms/epoch - 10ms/step\n",
      "Epoch 103/200\n",
      "2/2 - 0s - loss: 16.9432 - val_loss: 21.2714 - 20ms/epoch - 10ms/step\n",
      "Epoch 104/200\n",
      "2/2 - 0s - loss: 16.9419 - val_loss: 21.2940 - 20ms/epoch - 10ms/step\n",
      "Epoch 105/200\n",
      "2/2 - 0s - loss: 16.9326 - val_loss: 21.3650 - 19ms/epoch - 9ms/step\n",
      "Epoch 106/200\n",
      "2/2 - 0s - loss: 16.9211 - val_loss: 21.4443 - 20ms/epoch - 10ms/step\n",
      "Epoch 107/200\n",
      "2/2 - 0s - loss: 16.9004 - val_loss: 21.6035 - 19ms/epoch - 9ms/step\n",
      "Epoch 108/200\n",
      "2/2 - 0s - loss: 16.8839 - val_loss: 21.8700 - 20ms/epoch - 10ms/step\n",
      "Epoch 109/200\n",
      "2/2 - 0s - loss: 16.8812 - val_loss: 22.0800 - 19ms/epoch - 10ms/step\n",
      "Epoch 110/200\n",
      "2/2 - 0s - loss: 16.9281 - val_loss: 22.2567 - 19ms/epoch - 9ms/step\n",
      "Epoch 111/200\n",
      "2/2 - 0s - loss: 16.9412 - val_loss: 22.3539 - 20ms/epoch - 10ms/step\n",
      "Epoch 112/200\n",
      "2/2 - 0s - loss: 16.9491 - val_loss: 22.5064 - 19ms/epoch - 10ms/step\n",
      "Epoch 113/200\n",
      "2/2 - 0s - loss: 16.9886 - val_loss: 22.6685 - 19ms/epoch - 10ms/step\n",
      "Epoch 114/200\n",
      "2/2 - 0s - loss: 17.0609 - val_loss: 22.6710 - 19ms/epoch - 9ms/step\n",
      "Epoch 115/200\n",
      "2/2 - 0s - loss: 17.0483 - val_loss: 22.4646 - 20ms/epoch - 10ms/step\n",
      "Epoch 116/200\n",
      "2/2 - 0s - loss: 16.9413 - val_loss: 22.0815 - 20ms/epoch - 10ms/step\n",
      "Epoch 117/200\n",
      "2/2 - 0s - loss: 16.8413 - val_loss: 21.5247 - 20ms/epoch - 10ms/step\n",
      "Epoch 118/200\n",
      "2/2 - 0s - loss: 16.8008 - val_loss: 20.9470 - 20ms/epoch - 10ms/step\n",
      "Epoch 119/200\n",
      "2/2 - 0s - loss: 16.8165 - val_loss: 20.3213 - 20ms/epoch - 10ms/step\n",
      "Epoch 120/200\n",
      "2/2 - 0s - loss: 17.0623 - val_loss: 19.6705 - 20ms/epoch - 10ms/step\n",
      "Epoch 121/200\n",
      "2/2 - 0s - loss: 17.3102 - val_loss: 19.2755 - 19ms/epoch - 10ms/step\n",
      "Epoch 122/200\n",
      "2/2 - 0s - loss: 17.3567 - val_loss: 19.0504 - 20ms/epoch - 10ms/step\n",
      "Epoch 123/200\n",
      "2/2 - 0s - loss: 17.4614 - val_loss: 18.9451 - 20ms/epoch - 10ms/step\n",
      "Epoch 124/200\n",
      "2/2 - 0s - loss: 17.5051 - val_loss: 19.0653 - 19ms/epoch - 10ms/step\n",
      "Epoch 125/200\n",
      "2/2 - 0s - loss: 17.3985 - val_loss: 19.3617 - 19ms/epoch - 10ms/step\n",
      "Epoch 126/200\n",
      "2/2 - 0s - loss: 17.3056 - val_loss: 19.8795 - 19ms/epoch - 10ms/step\n",
      "Epoch 127/200\n",
      "2/2 - 0s - loss: 17.1912 - val_loss: 20.3465 - 20ms/epoch - 10ms/step\n",
      "Epoch 128/200\n",
      "2/2 - 0s - loss: 16.9620 - val_loss: 20.8132 - 20ms/epoch - 10ms/step\n",
      "Epoch 129/200\n",
      "2/2 - 0s - loss: 16.9466 - val_loss: 21.2758 - 20ms/epoch - 10ms/step\n",
      "Epoch 130/200\n",
      "2/2 - 0s - loss: 16.8257 - val_loss: 21.5157 - 19ms/epoch - 10ms/step\n",
      "Epoch 131/200\n",
      "2/2 - 0s - loss: 16.7600 - val_loss: 21.7300 - 20ms/epoch - 10ms/step\n",
      "Epoch 132/200\n",
      "2/2 - 0s - loss: 16.8166 - val_loss: 21.8760 - 20ms/epoch - 10ms/step\n",
      "Epoch 133/200\n",
      "2/2 - 0s - loss: 16.8104 - val_loss: 21.8827 - 20ms/epoch - 10ms/step\n",
      "Epoch 134/200\n",
      "2/2 - 0s - loss: 16.8060 - val_loss: 21.8977 - 20ms/epoch - 10ms/step\n",
      "Epoch 135/200\n",
      "2/2 - 0s - loss: 16.8073 - val_loss: 21.8878 - 20ms/epoch - 10ms/step\n",
      "Epoch 136/200\n",
      "2/2 - 0s - loss: 16.8066 - val_loss: 21.7906 - 19ms/epoch - 10ms/step\n",
      "Epoch 137/200\n",
      "2/2 - 0s - loss: 16.7662 - val_loss: 21.4949 - 19ms/epoch - 10ms/step\n",
      "Epoch 138/200\n",
      "2/2 - 0s - loss: 16.7280 - val_loss: 21.1806 - 20ms/epoch - 10ms/step\n",
      "Epoch 139/200\n",
      "2/2 - 0s - loss: 16.7517 - val_loss: 20.8194 - 19ms/epoch - 10ms/step\n",
      "Epoch 140/200\n",
      "2/2 - 0s - loss: 16.8458 - val_loss: 20.5398 - 20ms/epoch - 10ms/step\n",
      "Epoch 141/200\n",
      "2/2 - 0s - loss: 16.8812 - val_loss: 20.4078 - 20ms/epoch - 10ms/step\n",
      "Epoch 142/200\n",
      "2/2 - 0s - loss: 16.9306 - val_loss: 20.3057 - 20ms/epoch - 10ms/step\n",
      "Epoch 143/200\n",
      "2/2 - 0s - loss: 16.9501 - val_loss: 20.3163 - 20ms/epoch - 10ms/step\n",
      "Epoch 144/200\n",
      "2/2 - 0s - loss: 16.9312 - val_loss: 20.3023 - 19ms/epoch - 10ms/step\n",
      "Epoch 145/200\n",
      "2/2 - 0s - loss: 16.9286 - val_loss: 20.4344 - 20ms/epoch - 10ms/step\n",
      "Epoch 146/200\n",
      "2/2 - 0s - loss: 16.9183 - val_loss: 20.6546 - 20ms/epoch - 10ms/step\n",
      "Epoch 147/200\n",
      "2/2 - 0s - loss: 16.8135 - val_loss: 20.7887 - 20ms/epoch - 10ms/step\n",
      "Epoch 148/200\n",
      "2/2 - 0s - loss: 16.7829 - val_loss: 21.0073 - 20ms/epoch - 10ms/step\n",
      "Epoch 149/200\n",
      "2/2 - 0s - loss: 16.8166 - val_loss: 21.1861 - 20ms/epoch - 10ms/step\n",
      "Epoch 150/200\n",
      "2/2 - 0s - loss: 16.7323 - val_loss: 21.2042 - 20ms/epoch - 10ms/step\n",
      "Epoch 151/200\n",
      "2/2 - 0s - loss: 16.7317 - val_loss: 21.2084 - 19ms/epoch - 10ms/step\n",
      "Epoch 152/200\n",
      "2/2 - 0s - loss: 16.7201 - val_loss: 21.1932 - 20ms/epoch - 10ms/step\n",
      "Epoch 153/200\n",
      "2/2 - 0s - loss: 16.7210 - val_loss: 21.2032 - 19ms/epoch - 10ms/step\n",
      "Epoch 154/200\n",
      "2/2 - 0s - loss: 16.7110 - val_loss: 21.1882 - 20ms/epoch - 10ms/step\n",
      "Epoch 155/200\n",
      "2/2 - 0s - loss: 16.7068 - val_loss: 21.2672 - 20ms/epoch - 10ms/step\n",
      "Epoch 156/200\n",
      "2/2 - 0s - loss: 16.6842 - val_loss: 21.4815 - 20ms/epoch - 10ms/step\n",
      "Epoch 157/200\n",
      "2/2 - 0s - loss: 16.6614 - val_loss: 21.7101 - 20ms/epoch - 10ms/step\n",
      "Epoch 158/200\n",
      "2/2 - 0s - loss: 16.7110 - val_loss: 21.8611 - 20ms/epoch - 10ms/step\n",
      "Epoch 159/200\n",
      "2/2 - 0s - loss: 16.7257 - val_loss: 21.8580 - 20ms/epoch - 10ms/step\n",
      "Epoch 160/200\n",
      "2/2 - 0s - loss: 16.7145 - val_loss: 21.7155 - 19ms/epoch - 9ms/step\n",
      "Epoch 161/200\n",
      "2/2 - 0s - loss: 16.6952 - val_loss: 21.5036 - 19ms/epoch - 9ms/step\n",
      "Epoch 162/200\n",
      "2/2 - 0s - loss: 16.6425 - val_loss: 21.3204 - 20ms/epoch - 10ms/step\n",
      "Epoch 163/200\n",
      "2/2 - 0s - loss: 16.7168 - val_loss: 21.3380 - 21ms/epoch - 10ms/step\n",
      "Epoch 164/200\n",
      "2/2 - 0s - loss: 16.6601 - val_loss: 21.6125 - 20ms/epoch - 10ms/step\n",
      "Epoch 165/200\n",
      "2/2 - 0s - loss: 16.6512 - val_loss: 21.8094 - 20ms/epoch - 10ms/step\n",
      "Epoch 166/200\n",
      "2/2 - 0s - loss: 16.7060 - val_loss: 21.8999 - 19ms/epoch - 10ms/step\n",
      "Epoch 167/200\n",
      "2/2 - 0s - loss: 16.6958 - val_loss: 21.7650 - 19ms/epoch - 9ms/step\n",
      "Epoch 168/200\n",
      "2/2 - 0s - loss: 16.6644 - val_loss: 21.5336 - 20ms/epoch - 10ms/step\n",
      "Epoch 169/200\n",
      "2/2 - 0s - loss: 16.5889 - val_loss: 21.0444 - 19ms/epoch - 9ms/step\n",
      "Epoch 170/200\n",
      "2/2 - 0s - loss: 16.6541 - val_loss: 20.3846 - 19ms/epoch - 10ms/step\n",
      "Epoch 171/200\n",
      "2/2 - 0s - loss: 16.7627 - val_loss: 19.9348 - 19ms/epoch - 10ms/step\n",
      "Epoch 172/200\n",
      "2/2 - 0s - loss: 16.8807 - val_loss: 19.6541 - 20ms/epoch - 10ms/step\n",
      "Epoch 173/200\n",
      "2/2 - 0s - loss: 16.9501 - val_loss: 19.4140 - 21ms/epoch - 10ms/step\n",
      "Epoch 174/200\n",
      "2/2 - 0s - loss: 17.0324 - val_loss: 19.1894 - 20ms/epoch - 10ms/step\n",
      "Epoch 175/200\n",
      "2/2 - 0s - loss: 17.1113 - val_loss: 19.1517 - 20ms/epoch - 10ms/step\n",
      "Epoch 176/200\n",
      "2/2 - 0s - loss: 17.1203 - val_loss: 19.2682 - 20ms/epoch - 10ms/step\n",
      "Epoch 177/200\n",
      "2/2 - 0s - loss: 17.0445 - val_loss: 19.5833 - 29ms/epoch - 15ms/step\n",
      "Epoch 178/200\n",
      "2/2 - 0s - loss: 17.0392 - val_loss: 19.9314 - 22ms/epoch - 11ms/step\n",
      "Epoch 179/200\n",
      "2/2 - 0s - loss: 16.8490 - val_loss: 20.1250 - 20ms/epoch - 10ms/step\n",
      "Epoch 180/200\n",
      "2/2 - 0s - loss: 16.7577 - val_loss: 20.4655 - 21ms/epoch - 11ms/step\n",
      "Epoch 181/200\n",
      "2/2 - 0s - loss: 16.6434 - val_loss: 21.0112 - 20ms/epoch - 10ms/step\n",
      "Epoch 182/200\n",
      "2/2 - 0s - loss: 16.5994 - val_loss: 21.5951 - 21ms/epoch - 11ms/step\n",
      "Epoch 183/200\n",
      "2/2 - 0s - loss: 16.5529 - val_loss: 22.1284 - 20ms/epoch - 10ms/step\n",
      "Epoch 184/200\n",
      "2/2 - 0s - loss: 16.7313 - val_loss: 22.6307 - 19ms/epoch - 10ms/step\n",
      "Epoch 185/200\n",
      "2/2 - 0s - loss: 16.9933 - val_loss: 23.0004 - 21ms/epoch - 10ms/step\n",
      "Epoch 186/200\n",
      "2/2 - 0s - loss: 17.1608 - val_loss: 23.1415 - 20ms/epoch - 10ms/step\n",
      "Epoch 187/200\n",
      "2/2 - 0s - loss: 17.2183 - val_loss: 23.1316 - 20ms/epoch - 10ms/step\n",
      "Epoch 188/200\n",
      "2/2 - 0s - loss: 17.2078 - val_loss: 23.0036 - 21ms/epoch - 11ms/step\n",
      "Epoch 189/200\n",
      "2/2 - 0s - loss: 17.1066 - val_loss: 22.5071 - 21ms/epoch - 11ms/step\n",
      "Epoch 190/200\n",
      "2/2 - 0s - loss: 16.8561 - val_loss: 21.6255 - 20ms/epoch - 10ms/step\n",
      "Epoch 191/200\n",
      "2/2 - 0s - loss: 16.6508 - val_loss: 20.8247 - 21ms/epoch - 10ms/step\n",
      "Epoch 192/200\n",
      "2/2 - 0s - loss: 16.6318 - val_loss: 20.3499 - 20ms/epoch - 10ms/step\n",
      "Epoch 193/200\n",
      "2/2 - 0s - loss: 16.7114 - val_loss: 20.2193 - 20ms/epoch - 10ms/step\n",
      "Epoch 194/200\n",
      "2/2 - 0s - loss: 16.6579 - val_loss: 20.4903 - 21ms/epoch - 11ms/step\n",
      "Epoch 195/200\n",
      "2/2 - 0s - loss: 16.6484 - val_loss: 20.8454 - 20ms/epoch - 10ms/step\n",
      "Epoch 196/200\n",
      "2/2 - 0s - loss: 16.5766 - val_loss: 21.0638 - 20ms/epoch - 10ms/step\n",
      "Epoch 197/200\n",
      "2/2 - 0s - loss: 16.5259 - val_loss: 21.2732 - 20ms/epoch - 10ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      "2/2 - 0s - loss: 16.5027 - val_loss: 21.5453 - 20ms/epoch - 10ms/step\n",
      "Epoch 199/200\n",
      "2/2 - 0s - loss: 16.5388 - val_loss: 21.8096 - 19ms/epoch - 10ms/step\n",
      "Epoch 200/200\n",
      "2/2 - 0s - loss: 16.6381 - val_loss: 21.8189 - 21ms/epoch - 10ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train.values, Y_train, validation_data=(X_valid.values, Y_valid), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4b9393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rmse: 19.535512013441704\n",
      "Validation rmse: 23.51662505167784\n"
     ]
    }
   ],
   "source": [
    "mlp_train_pred = model.predict(X_train.values)\n",
    "mlp_valid_pred = model.predict(X_valid.values)\n",
    "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, mlp_train_pred)))\n",
    "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, mlp_valid_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b4892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
